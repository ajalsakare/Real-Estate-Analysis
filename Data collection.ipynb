{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be47b35",
   "metadata": {},
   "source": [
    "<center><h2> Data collection </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4ffa0",
   "metadata": {},
   "source": [
    "For the data collection part we've scraped <a href='https://www.magicbricks.com/'>MagicBricks</a> which is a platform for property buyers & sellers.<br>\n",
    "### Tech stack used:\n",
    "* Python\n",
    "* Beautiful Soup\n",
    "* Selenium\n",
    "* Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626d2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# path where required driver file is stored\n",
    "path = 'C:\\\\Users\\\\anike\\\\PycharmProjects\\\\Web Scraping\\\\chromedriver.exe'\n",
    "\n",
    "# OOPS implementation\n",
    "class MagicBricksScraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        i = 0,\n",
    "        c = 'Pune'\n",
    "        #self.driver_path = ''\n",
    "        self.driver = ''\n",
    "        self.final_data = []\n",
    "        self.URL = \"https://www.magicbricks.com/property-for-sale/residential-real-estate?proptype=Multistorey-Apartment,\" \\\n",
    "                   \"Builder-Floor-Apartment,Penthouse,Studio-Apartment,Residential-House,Villa&page=\" + str(\n",
    "            i) + \"&cityName=\" + str(c)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Loads the website\"\"\"\n",
    "        \n",
    "        self.driver = webdriver.Chrome(executable_path=path)\n",
    "        self.driver.get('https://www.magicbricks.com')\n",
    "        self.driver.maximize_window()\n",
    "\n",
    "    def extract(self, cities, pages):\n",
    "        \"\"\"Extracts all cards available on the current loaded page\n",
    "\n",
    "        Arguments:\n",
    "            cities -- list of cities for which to extract data\n",
    "            pages -- number of pages to search for each city\n",
    "\n",
    "        Returns:\n",
    "            final_data -- list of dictionaries containing extracted data\n",
    "        \"\"\"\n",
    "\n",
    "        for c in cities:\n",
    "            \n",
    "            for i in range(pages):\n",
    "                \n",
    "                self.driver.get(\n",
    "                    \"https://www.magicbricks.com/property-for-sale/residential-real-estate?proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment,Residential-House,Villa&page=\"+str(i)+\"&cityName=\"+str(c))\n",
    "                print(f'City: {c}, page: {i}')\n",
    "                \n",
    "                # extract page source\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                \n",
    "                # find all div tags inside source\n",
    "                div_tags = soup.find_all('div', 'mb-srp__list')\n",
    "                \n",
    "                self.get_data(div_tags, c)\n",
    "\n",
    "        return self.final_data\n",
    "\n",
    "    def get_data(self, div_tags, city):\n",
    "        \"\"\"Extracts all data from a card\"\"\"\n",
    "\n",
    "        for d in div_tags:\n",
    "            # empty dictionary to store a record\n",
    "            data_dic = {}\n",
    "\n",
    "            try:\n",
    "                # title of the card\n",
    "                title = d.find_all('h2', 'mb-srp__card--title')[0].text\n",
    "\n",
    "                # society name\n",
    "                society_name = d.find_all('a', 'mb-srp__card__society--name')[0].text\n",
    "\n",
    "                # price of property\n",
    "                price = d.find_all('div', 'mb-srp__card__price--amount')[0].text\n",
    "\n",
    "            except IndexError:\n",
    "                return\n",
    "\n",
    "            # label and values\n",
    "            label = d.find_all('div', 'mb-srp__card__summary--label')\n",
    "            value = d.find_all('div', 'mb-srp__card__summary--value')\n",
    "\n",
    "            prop_id = d.get('id')\n",
    "\n",
    "            # metadata\n",
    "            meta = d.find_all('meta')\n",
    "\n",
    "            name = ''\n",
    "            description = ''\n",
    "            url = ''\n",
    "            address = ''\n",
    "            longitude = ''\n",
    "            latitude = ''\n",
    "            number_of_rooms = ''\n",
    "\n",
    "            for m in meta:\n",
    "                if m['itemprop'] == 'name':\n",
    "                    name = m['content']\n",
    "                if m['itemprop'] == 'description':\n",
    "                    description = m['content']\n",
    "                if m['itemprop'] == 'url':\n",
    "                    url = m['content']\n",
    "                if m['itemprop'] == 'addressLocality':\n",
    "                    address = m['content']\n",
    "                if m['itemprop'] == 'longitude':\n",
    "                    longitude = m['content']\n",
    "                if m['itemprop'] == 'latitude':\n",
    "                    latitude = m['content']\n",
    "                if m['itemprop'] == 'number_of_rooms':\n",
    "                    number_of_rooms = m['content']\n",
    "\n",
    "            data_dic = {'Property Id': prop_id,\n",
    "                        'Title': title,\n",
    "                        'Society Name': society_name,\n",
    "                        'Price': price,\n",
    "                        'Name': name,\n",
    "                        'Description': description,\n",
    "                        'URL': url,\n",
    "                        'Address': address,\n",
    "                        'City': city,\n",
    "                        'Latitude': latitude,\n",
    "                        'Longitude': longitude,\n",
    "                        'Number of Rooms': number_of_rooms}\n",
    "\n",
    "            # creating key-value pair using label and values\n",
    "            for i in range(len(label)):\n",
    "                data_dic[label[i].text] = value[i].text\n",
    "                \n",
    "\n",
    "            self.final_data.append(data_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7118f8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Pune, page: 0\n",
      "City: Pune, page: 1\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "cities = ['Gurgaon', 'Noida', 'Ghaziabad', 'Greater-Noida', 'Bangalore', 'Mumbai', 'Pune', 'Hyderabad', 'Kolkata',\n",
    "          'Chennai', 'New-Delhi', 'Ahmedabad', 'Navi-Mumbai', 'Thane', 'Faridabad', 'Bhubaneswar', 'Bokaro-Steel-City',\n",
    "          'Vijayawada', 'Vrindavan', 'Bhopal', 'Gorakhpur', 'Jamshedpur', 'Agra', 'Allahabad', 'Jodhpur', 'Aurangabad',\n",
    "          'Jaipur', 'Mangalore', 'Nagpur', 'Guntur', 'Navsari', 'Palghar', 'Salem', 'Haridwar', 'Durgapur', 'Madurai',\n",
    "          'Manipal', 'Patna', 'Ranchi', 'Raipur', 'Sonipat', 'Kottayam', 'Kozhikode', 'Thrissur', 'Tirupati', 'Trivandrum',\n",
    "          'Trichy', 'Udaipur', 'Vapi', 'Varanasi', 'Vadodara', 'Visakhapatnam', 'Surat', 'Kanpur', 'Kochi', 'Mysore', 'Goa',\n",
    "          'Bhiwandi', 'Lucknow', 'Nashik', 'Guwahati', 'Chandigarh', 'Indore', 'Coimbatore', 'Dehradun', 'Latur', 'Gandhinagar',\n",
    "          'Meerut', 'Warangal', 'Bikaner']\n",
    "\n",
    "path = 'chromedriver.exe'\n",
    "obj = MagicBricksScraper()\n",
    "obj.load(path)\n",
    "final_data = obj.extract(['Pune'], 2)  # pass the cities list and no. of pages\n",
    "print(len(final_data))\n",
    "# print(final_data)\n",
    "\n",
    "ext1 = '.xlsx'\n",
    "ext2 = '.csv'\n",
    "name = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "path1 = 'MagicBricks-' + name + ext1\n",
    "# path2 = 'MagicBricks-' + name + ext2\n",
    "\n",
    "df = pd.DataFrame(final_data)\n",
    "df.to_excel(path1, index=False)\n",
    "# df.to_csv(path2, index=False)\n",
    "obj.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029a0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
